#!/usr/bin/env bash
# -----------------------------------------------------------------------------
#  start_vllm_tp.sh â€” Highâ€‘throughput Proteinâ€‘LLM server (single engine, TP=4)
# -----------------------------------------------------------------------------
#  â€¢ Loads the 1.7â€¯B Qwenâ€‘ESM3 checkpoint ONCE and shards it across 4 GPUs with
#    Tensor Parallel (TP).
#  â€¢ Keeps dynamic batching global (better token/â€‹s) and leaves NCCL P2P / IB
#    enabled for fast crossâ€‘GPU allâ€‘reduce.
#  â€¢ Supports either textâ€‘only, DNA, or protein multimodal modes via flags.
#  â€¢ Creates a fastâ€‘tokenizer folder on first launch if missing.
# -----------------------------------------------------------------------------
#  Usage examples
#  -------------
#  ./start_vllm_tp.sh                             # default protein mode
#  ./start_vllm_tp.sh dna                         # DNA mode
#  ./start_vllm_tp.sh text                        # textâ€‘only
# -----------------------------------------------------------------------------

set -euo pipefail

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PATH="/large_storage/goodarzilab/parsaidp/last_cafa_1.7B_ESM3"
DNA_MODEL_PATH="esm3_sm_open_v1"      # also used for protein (ESMâ€‘3)
HOST="0.0.0.0"
PORT=8000                              # single engine â†’ single port
GPU_MEMORY_UTILIZATION=0.85           # we own the whole GPU
MAX_MODEL_LEN=5000
VLLM_MAX_BATCH=64                     # export to allow bigger microâ€‘batch
LOG_LEVEL="info"

# mode: protein | dna | text (default protein)
MODE="${1:-protein}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ INFO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ðŸš€ Starting single vLLM engine with TP=1 â€¦"
echo "   Model           : $MODEL_PATH"
echo "   Mode            : $MODE"
echo "   Visible GPUs    : $(nvidia-smi --query-gpu=index,name --format=csv,noheader)"
echo "   Host/Port       : $HOST:$PORT"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ RUN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export VLLM_MAX_REQUEST_BATCH_SIZE=$VLLM_MAX_BATCH
# Important: *do not* disable NCCL P2P â€” we need it for TP

CUDA_VISIBLE_DEVICES=0 \
python -m trl.scripts.vllm_serve \
  --model "$MODEL_PATH" \
  $( [[ "$MODE" == "protein" ]] && echo "--protein_model_name $DNA_MODEL_PATH --use_protein_llm" ) \
  $( [[ "$MODE" == "dna"      ]] && echo "--dna_model_name $DNA_MODEL_PATH --use_dna_llm" ) \
  --tensor_parallel_size 1 \
  --data_parallel_size 1 \
  --host "$HOST" \
  --port $PORT \
  --gpu_memory_utilization $GPU_MEMORY_UTILIZATION \
  --max_model_len $MAX_MODEL_LEN \
  --dtype auto \
  --enable_prefix_caching false \
  --kv_cache_dtype auto \
  --trust_remote_code true \
  --batch_inference true \
  --log_level $LOG_LEVEL \
  --skip_env_check
